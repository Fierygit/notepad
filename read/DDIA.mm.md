---
title: DDIA
date: 2020-12-3
categories: read
tag: DDIA
---



# 数据密集型应用学习笔记







## 第1章

>  当今许多应用大多是数据数据密集（data-intensive）而不是计算密集型（compute-intensive）的。所以CPU的处理能力往往不是应用程序的瓶颈。关键在于数据的量、数据的复杂度以及数据的快速多变性。

- 应用往往包含以下模块：

1. 数据库：用于存储数据
2. 高速缓存：缓存复杂或者操作代价昂贵的结果，加快下一次访问
3. 索引：用户可以按照关键字搜索数据并支持各种过滤
4. 流式处理：持续发送消息到另一个进程，处理采用异步方式
5. 批处理：定期处理大量积累的数据

### 认识数据系统

![一个数据架构系统，包含了不同的组件](https://raw.githubusercontent.com/Fierygit/picbed/master/20201205234835.png)

### 可靠性

> 出现意外情况，比如硬件、软件故障、人为失误等，系统可以继续正常运转，至少确保功能正确。

#### 硬件故障

> 比较容易出现的，硬盘崩溃，内存故障，电网停电。
>
> 第一反应是为硬件冗余来减少系统故障率。例如磁盘RAID，服务器双电源，甚至热插拔CPU，数据中心添加备用电源、发电机。
>
> 这样当一个组件发生故障时，冗余组件可以快速接管，之后运维人员可以修复或者更换坏掉的组件。
>
> 直到最近，采用硬件冗余方案对于大多数应用场景还是足够的，它让单机完全失效的概率降到最低。只要可以把备份迅速恢复到新的机器上，故障的停机时间在大多数应用中并不是灾难性的。
>
> 现在，通过软件容错的方式来容易多机失效成为新的手段，或者成为硬件容错方案的有力补充。例如滚动升级。

#### 软件错误

> 这类故障更难预料。各个节点直接是由软件关联的，可能会导致更多的系统故障。

- 例如，

1. 由于软件错误，导致特定的输入引发应用的崩溃。例如Linux内核bug，在2012年6月30的闰秒时候触发，导致很多应用程序被挂掉。
2. 失控的进程把系统的资源耗尽，导致这些共享资源不能被释放。
3. 系统的Dependency出了问题，返回值异常。
4. 组件中的小故障触发另一个组件中的故障，进而触发更多的故障。

- 没有快速的解决方法。只能仔细考虑很多细节。

1. 检查系统的假设条件和系统之间的交互
2. 进行全面的测试
3. 进程隔离，
4. 允许进程崩溃后自动重启
5. 反复评估、监控并分析生产环境中的行为表现。

例如消息队列中，输出消息的数量应等于输入消息的数量。如果发现不一致，则立即告警。

#### 人为失误

> 人无法做到万无一失。运维人员的配置错误可能是系统下线的第一大原因。
>
> 要保证系统可靠，如何减少人为错误对它的影响？

1. 用最小出错的方式来设计系统。让做错事更难。
2. 想办法分离最容易出错的地方，容易引发故障的接口。使用Sandbox隔离真正的生产和测试环境。
3. 充分的测试。单元测试，集成测试，手动测试。边界条件的考虑。
4. 当出现人为失误时，有快速回滚或者回复的机制。滚动发布新代码。
5. 监控子系统需要详细和清晰。
6. 推行管理流程和相关培训。

### 可扩展性

> 随着规模的增长，例如数据量、流量和复杂性，系统应该可以用合理的方式进行应对，满足这种增长。
>
> 当应用负载增加的时候，比如用户从1w到100w，从100w到1000w，应用程序如何应对增长的负载。
>
> 相关参数：Web服务的QPS，数据库的写入比例，DAU，缓存命中率。有时候平均值很重要，有时候短时间内的峰值会成为系统瓶颈。
>
> Twitter的Fan-out结构，对数据量提出了挑战。当一个人发Tweet时候，怎么处理Timeline这个请求。根据粉丝的数量，区别处理。

#### 描述性能

- 系统负载增加后，会发生什么，两种思考方式

1. 系统资源不变（CPU，内存，带宽），系统的性能会发生什么变化？
2. 如果要保持性能不变，需要增加多少资源？

- 不同类型的系统关心的性能指标不同

1. 批处理系统通常关心**吞吐量（throughput）**，例如Hadoop，每秒可以处理多少条数据或者完成一个作业总共需要多少时间。
2. Online系统中，更看重服务的**响应时间（response time）**，即客户端从发出请求到得到回复的总时间。

[![展示了一个服务100次请求响应时间的均值与百分位数、中位数](https://guoyongxin.github.io/2019/04/20/DDIA-1-%E5%8F%AF%E9%9D%A0%E5%8F%AF%E6%89%A9%E5%B1%95%E5%8F%AF%E7%BB%B4%E6%8A%A4%E7%9A%84%E5%BA%94%E7%94%A8%E7%B3%BB%E7%BB%9F/%E5%93%8D%E5%BA%94%E6%97%B6%E9%97%B4.png)展示了一个服务100次请求响应时间的均值与百分位数、中位数](https://guoyongxin.github.io/2019/04/20/DDIA-1-可靠可扩展可维护的应用系统/响应时间.png)

- 对于响应时间，如下图，有一些很长的，算异常请求，可能是由于数据大很多。但也有可能是其他因素造成的，例如上下文切换、进程调度、网络丢包、TCP重传、垃圾回收STW，缺页中断、磁盘IO。

- 最好使用百分位数，中位数（50%）来评估系统的响应时间。

- 采用较高的响应时间百分位数很重要，因为直接影响用户的总体服务体验。例如亚马逊采用99.9百分位来定义服务响应时间。优化99.9%的目标可能成本很高。能不能带来收益很关键。

- 排队延迟（queueing delay）通常占了高百分位点处响应时间的很大一部分。由于服务器只能并行处理少量的事务（如受其CPU核数的限制），所以只要有少量缓慢的请求就能阻碍后续请求的处理，这种效应有时被称为头部阻塞（head-of-line blocking）

#### 应对负载增加

- 垂直扩展和水平扩展。

- 好的系统有弹性特征，可以自动检测负载的变化，来自动添加更多的计算资源。

- 可扩展架构通常都是从通用模块逐步构建出来的。

### 可维护性

> 项目会随着时间的推移，项目会需要新的人员参与到开发和运维工作中，来满足系统的稳定和新场景的适应。系统应该高效的变化。
>
> 软件的大部分开销并不在最初的开发阶段，而是在持续的维护阶段，包括修复漏洞、保持系统正常运行、调查失效、适配新的平台、为新的场景进行修改、偿还技术债、添加新的功能等等。
>
> 为此，我们将特别关注软件系统的三个设计原则：



#### 可操作性（Operability）

>  便于运维团队保持系统平稳运行。
>
> 良好的可操作性意味着更轻松的日常工作，进而运维团队能专注于高价值的事情。
>
> 数据系统可以通过各种方式使日常任务更轻松

#### 简单性（Simplicity）

- 从系统中消除尽可能多的复杂度（complexity），使新工程师也能轻松理解系统。（注意这和用户接口的简单性不一样。）
  **复杂度（complexity）**有各种可能的症状，例如：状态空间激增、模块间紧密耦合、纠结的依赖关系、不一致的命名和术语、解决性能问题的Hack、需要绕开的特例等等，

#### 可演化性（evolability）

-  使工程师在未来能轻松地对系统进行更改，当需求变化时为新应用场景做适配。也称为可扩展性（extensibility），可修改性（modifiability）或可塑性（plasticity）。

- 组织流程方面，敏捷开发，TDD，重构。

- 修改数据系统并使其适应不断变化需求的容易程度，是与简单性和抽象性密切相关的：简单易懂的系统通常比复杂系统更容易修改



## 第3章

### 数据库核心： 数据结构

> 一个最简单的数据库

``` bash
#!/bin/bash
db_set () {
	echo "$1,$2" >> database
}
db_get () {
	grep "^$1," database | sed -e "s/^$1,//" | tail -n 1
}
```

- db_set
  - 追加到文件尾部方式通常足够高效， 因而db_set函数性能很好。
  - 与db_set相似， 许多数据库内部都使用日志(log) , 日志是一个仅支持追加式更新的数据文件

- db_get
  - db_get函数的性能会非常差。
  - 每次想查找一个键， db_get必须从头到尾扫描整个数据库文件来查找键的出现位置, 查找的开销是O(n)。

如何解决查找缓慢？ 

#### 哈希索引

> 可以提供高性能的读和写， 只要所有的key可以放入内存（因为hash map需要保存在内存中）

- 引入新的问题
  - 哈希表必须全部放入内存， 如果有大量的键， 就没那么幸运了。
  - 很难使磁盘上的hash map表现良好，需要大量的随机访问I/0。
  - 当哈希变满时，增长代价昂贵，并且哈希冲突时需要复杂的处理逻辑。
  - 区间查询效率不高。不能简单地扫描a和z区间内的所有键，只能采用逐个查找的方式查询每一个键。

##### 总结

- 使用限制，用作存储点击量，访问量

#### SSTable和 LSM-Tree

> **键值对的序列按键排序**。这就是**SSTables（Sorted String Tables）**的数据格式。我们还要求同一个key只会出现在一个段中。

##### SSTables优点：

1. 合并段更简单。就像merge sort一样。
2. 查找数据使用二分查找
3. 压缩时，可以利用稀疏索引，降低了IO带宽。

##### 构建和维护SSTables

要解决排序问题。方法是，

1. 在内存中保存一个排序结构，比如红黑树，AVL树。
2. 在插入修改时候可以很快的响应。
3. 可以顺序的读区它们。

##### 存储引擎工作流程

1. 写入时，将数据加入内存表中，可以是红黑树实现。
2. 当内存中的红黑树大小超过阀值时，把它用SSTable的格式写入磁盘。
3. 处理读请求时，先尝试查询内存表，如果miss就查询磁盘段文件s。
4. 周期性的执行合并和压缩。丢弃被覆盖和删除的值。

##### 实现LSM-Tree（Log-structured Merge-Tree）

- LevelDB，RocksDB，HBase， 都源于Google的BigTable论文。最初这个索引结构在早起的系统中被命名LSM-Tree。因此，基于合并和压缩的排序文件原理的存储引擎，通常都被称作LSM存储引擎。

- 全文搜索，Lucene是ElasticSearch和Solr的索引引擎。采用了类似的方法保存字典。全文索引复杂的多，但想法类似。

##### 性能优化

1. 查询不存在的key时，会从内存开始扫描到磁盘的最后一个段。解决方法是，Bloom Filter。
2. 压缩合并的时机。分为大小分级和分层压缩两个方法。一个是小的SSTables被连续合并到大的旧的SSTables。另一个是key的范围分裂成多个更小的SSTables，旧数据被移动到单独的层级。

##### 总结

- 由于数据是按照排序存储，因此可以高效的执行区间查询。
- 因为磁盘是顺序写入的，LSM-Tree的写入吞吐量可以非常高。

#### B-trees索引

> 应用最广泛的索引结构。和SSTable一样，B-tree保留按key排，也可以实现高效的范围查询。
>
> B-tree将数据库分解成固定大小的块和页（4KB or more）。这种设计更接近底层硬件，因为磁盘也是固定大小的块的排列。
>
> 每个页可以用地址标志，是磁盘地址，而不是内存。这样可以用这些页面引用构造一个树状页面进行索引。索引的根是一个页面，之后的查找根据地址，读取相应的页。
>
> 分支因子：大多数数据库的索引适合3-4层的B-tree.因此不需要遍历非常深的页面层次即可找到所需的页。
>
> -  B树：多路搜索树，每个结点存储M/2到M个关键字，非叶子结点存储指向关键字范围的子结点；所有关键字在整颗树中出现，且只出现一次，非叶子结点可以命中；
> - B+树：在B-树基础上，为叶子结点增加链表指针，所有关键字都在叶子结点中出现，非叶子结点作为叶子结点的索引；B+树总是到叶子结点才命中；

##### 使B-tree可靠

- B-tree底层的基本操作是使用新数据覆盖磁盘的旧页。磁盘是覆盖扇区，对于SSD，擦除和重写的存储芯片块很大，情况更复杂。

- 页面溢出，需要分裂页时，也要覆盖其父页对更新后的两个子页的引用。属于复杂操作。在完成更新前发生崩溃，可能会产生孤儿页面。

- 崩溃恢复，使用redo Log。写数据之前先写日志。

- 并发控制。

##### 优化B-tree

1. 一些数据库不是用覆盖页，而是做复制。
2. 保存key的缩略信息而不是完整的key，来节省空间。只需要提供足够的信息来描述key的起止范围。
3. 页可以存在磁盘的任何位置。可能回有随机的IO，而不是连续的。有些B-tree尝试实现对B-tree进行布局，但是随着树的增长，这个顺序会越来越难维护。
4. 添加额外指针。左到右的指针，加速遍历。

#### 对比B-tree, LSM-tree

> 根据经验，LSM-Tree写入更快，而B-tree读更快。读取通常在LSM—Tree中较慢，因为要检查多个不同的数据结构和SSTables。

##### LSM-Tree优点

1. LSM只写入一次数据（不考虑写放大（写入引起的压缩和合并）），而B-tree写入两次（一次redo log，一次数据本身）。
2. LSM可以成熟比B-tree更大的吞吐量。有时具有较低的写放大，顺序写入速度快。
3. 可以支持更好的压缩，文件比B-tree小很多。没有B-tree产生碎片的问题。

##### LSM-Tree缺点

1. 响应延迟不确定，因为压缩和合并。
2. 由于配置问题，会出现压缩跟不上写入速度的问题。来不及合并，直到磁盘空间不足。
3. 事务支持不如B-tree

#### 其他索引

##### 在索引中存储值

>索引中存储行或则行的具体位置（堆文件法）。将索引行直接存在索引中，聚集索引。MySQL的InnoDB存储引擎中，表的主键是聚集索引，二级索引引用主键。

##### 多列索引

- 级联索引，通过将一列追加到另一列，将几个字段组合成一个键。只能从前到后匹配。B-tree和LSM-tree都无法高效的应对这种查询。

- 更常见的索引空间，R树。PostGIS使用PostgreSQL的广义搜索书索引实现了地理空间索引作为R树。

##### 全文搜索和模糊索引

> 之前的搜索都是准确匹配，而不能应对类似的key的搜索，例如错误的拼写。
>
> Lucene引擎支持在某个编辑距离内的模糊搜索。LevelDB中这个内存中的索引是一些key的稀疏集合。但在Lucene中，内存中的索引是key中的字符串序列的有限状态机，类似字典树。这个自动机可以转换成Leveshtein自动机，支持编辑距离内的搜索。

##### 在内存中保存所有内容

- 内存数据库。例如Memcached，做缓存。数据在重启后可以恢复。

- 内存数据可以更快的原因，是因为它们可以避免使用写磁盘的格式对内存数据结构编码的开销。

- 提供了给予磁盘索引难以实现的数据结构，例如Redis中的优先级队列和集合。

- 可以使用反缓存的方法，当没有足够的内存时，将一部分不常用数据倒入磁盘，类似操作系统的虚拟内存。

- 将来的NVM（non-volatile memory）技术广泛的普及，也可能很大的改变存储引擎的设计。

### 事务处理OLTP与分析处理OLAP

> 事务意味着允许客户端进行低延迟读区和写入，相比于只能周期性的运行的批处理作业。事务不一定具有ACID属性。
>
> - OLTP每次返回<font color='red'>少量的数据</font>，随机访问，低延迟要求。
>
> - OLAP对大量数据访问，批量导入（ETL）或事件流，内部分析师，为决策提供支持。

#### 数据仓库

>数据仓库是一个单独的数据库，分析师可以在不影响OLTP的情况下，任意使用数据仓库。数据仓库包含公司所有OLTP数据库的只读副本。
>
>单独使用数据仓库的优势在于数据仓库可以针对分析访问模式进行优化。本文前半部分讨论的索引模型只适合与OLTP而不适合做分析查询。

##### OLTP数据库和数据仓库的差异

- 数据仓库也支持SQL查询接口，但是和OLTP的实现差异很大。

- 一些数据库（SQL Server和SAP HANA）在同一产品中支持事务处理和数据仓库。然而，它们是两个独立的存储和查询引擎，只是通过一个SQL接口来访问。

- 一些商用的数据仓库，Teradata，Vertica，SAP HANA等很贵。还有开源的基于Hadoop的SQL项目，例如Apache Hive，Spark SQL，Cloudera Impala，Facebook Presto，Apache Tajo和Apache Drill. 其中一些是基于Google Dremel而构建的。

#### 星型与雪花型分析模式

> 许多数据仓库都使用了星型模式，也称为维度建模。
>
> 这种模式的中心是一个所谓的事实表。事实表的每一行表示在特定时间发生的事件。

![](https://raw.githubusercontent.com/Fierygit/picbed/master/20201205232229.png)

- 通常，事实被捕获为单独的事件，这样之后的分析具有很大的灵活性。

- 事实表中的列是属性，其他列可能会引用其他表的外键，成为维度表，这些维度代表事件的发生地点，时间，方式和原因。

- 名称**星型模式**来源于关系表可视化的适合，事实表位于中间，被一系列维度表包围。

- 该模型的一个变体成为**雪花模型**，其中维度进一步细分为子空间。例如，dimproduct表中的每一行可以再次向外引用品牌和类型的外键。这样更规范，但是更复杂。分析人员一般首选星型。

- 典型的数据仓库中，表都非常宽，事实表通常超过100列，甚至几百列。维度表也可能很宽。



### 列式存储

1. 主要关注事实表的海量数据问题，通常有万亿行、PB级别的数据。

2. 虽然通常事实表超过100列，但是一般一次分析也只会访问其中的4，5列。如何高效的执行这中类型的查询？

3. OLTP系统中，数据库的存储都是面向行的。如果属性超过100列，那么需要把很多不需要的数据读入内存，然后丢弃。非常低效。

4. 面向列存储，不是将一行的内容存在一起，而是把每一列的所有值存在一起。

#### 列压缩

> 面向列的存储非常适合压缩。一种技术是位图编码。

![](https://raw.githubusercontent.com/Fierygit/picbed/master/20201205232510.png)

- 每一个不同的值一个位图，位图的位数是行数。

- Bigtable模型仍然主要是面向行的。

##### 内存带宽和矢量化处理

- 除了减少需要从磁盘加载的数据量之外，列存储也有利于高效利用CPU的周期性。

#### 列存储中的排序

- 列的存储如果是按照某个常见的顺序，例如date，就可以做类似于SSTables的索引机制。注意单独排序某列没用，需要正行排序。

-- 数据仓库管理员需要基于经验选择合适的排序列，可以单列也可以是多列。这样查询优化器可以更高效。

- 另一个好处是可以进行压缩。可以进行游程编码，位图那样。

##### 几种不同的排序

1. C-Store的改进。用不同的方式存储相同的数据。使不同的排序查询都获益。也就是通过排序后的冗余数据加速。
2. 列排序，类似于面向行的二级索引。区别是，列的索引中，存的是值而不是地址。

#### 列存储与写操作

- 上述的优化，都是对读的优化，这会让写变得更困难。类似B-tree的就地更新的操作，对压缩列是不可能的。

- 一个方案是类似LSM-tree。先写入内存的排序数据结构，然后在一定的时候把内存的数据顺序的倒入磁盘，接着进行有可能的文件合并。这样查询的时候需要检查内存中的数据，和磁盘中的数据。这对于查询方是透明的。

#### 聚合：数据立方体和物化视图

- 数据仓库不是一定要用列存储的。但是列存储因为查询分析更快，所以正在迅速普及。数据仓库另一个方面是物化聚合，就是把常用的查询物理存储化，缓存一些查询结果。

- 实现：物化视图。 物化视图的常见特例称为数据立方体或OLAP立方。它是按不同维度分组的聚合网格。以沿着每行或每列应用相同的汇总，并获得一个维度减少的汇总（按产品的销售额，无论日期，还是按日期销售，无论产品如何）。

- 缺点是数据立方体不具有查询原始数据的灵活性。因此，大多数数据仓库试图保留尽可能多的原始数据，并将聚合数据（如数据立方体）仅用作某些查询的性能提升。







##  第9章 一致性与共识

>  分布式最庸的抽象之一就是共识： 所有的节点就某一项提议达成一致.

### 可线性化

> 也称为原子一致性， 强一致性。
>
> 其基本的想法是让一个系统看起来好像只有一个数据副本， 且所有的操作都是原子的。有了这个保证， 应用程序就不需要关心系统内部的多个副本。



#### 如何达到线性化？

> 可线性化背后的基本思想很简单：使系统看起来好像只有一个数据副本。



>**可线性化与可串行化**
>
>- 可串行化
>
>  可串行化是事务的隔离属性， 其中每个事务可以读写多个对象（行， 文档， 记录等。它用来确保事务执行的结果<font color='red'>与串行执行（即每次执行一个事务）的结果完全相同</font>， 即使串行执行的顺序可能与事务实际执行顺序不同 。
>
>- 可线性化
>
>  可线性化是<font color='red'>读写寄存器（单个对象）的最新值保证</font>。它并不要求将操作组合到事务中， 因此无法避免写倾针等问题 除非采取其他额外措施。



#### 线性化的依赖条件

- 加锁与主节点选取
- 约束与唯一性保证
- 跨通道的时间依赖



#### 实现线性化系统

- 主从复制（部分支持可线性化）

  在主从复制的系统中（参阅第5章的“主节点与从节点") 只有主节点承担数据写入， 从节点则在各自节点上维护数据的备份副本。**如果从主节点或者同步更新的从节点上读取， 则可以满足线性化。**但并非每个主从复制的具体数据库实例都是可线性化的， 主要是因为它们可能采用了快照隔离的设计， 或者实现时存在并发方面的bug 。

  而从主节点上读取的前提是你确定知道哪个节点是主节点。正如在第8章“ 真相由多数决定” 中所讨论的， 某节点可能自认为是主节点， 但事实并非如此， 这个“自以为是＂ 的主节点如果对外提供服务， 就会违反线性。

  如果使用了异步复制， 故障切换过程中甚至可能会丢失一些已提交的写入，结果是同时违反持久性和线性化。

- 共识算法（可线性化）

  与主从复制机制相似。不过共识协议通常内置一些措施来防止裂脑和过期的副本。正是由于这些专门的设计， 共识算法可以安全地实现线性化存储， 这些系统包括Zo0Keepe r l2 '1和etcdl22 」等。

- 多主复制（不可线性化）

  具有多主节点复制的系统通常无法线性化的， 主要由于它们同时在多个节点上执行并发写入， 并将数据异步复制到其他节点。因此它们可能会产生冲突的写入，需要额外的解决方案。这类冲突其实正是多副本所引入的结果。

- 无主复制（可能不可线性化）

  对千无主节点复制的系统（即Dynamo风格， 参阅第5章的“无主节点复制) 有些入认为只要配置法定读取和写入满足(w+r>n)就可以获得“ 强一致性” 。但这完全取决千具体的quorum的配置， 以及如何定义强一致性， 它可能并不保证线性化。
  
  例如基于墙上时钟（包括Cassandra, 参阅第8章“ 依赖于同步的时钟,,)的“ 最后写入获胜＂ 冲突解决方法几乎肯定是非线性化， 因为这种时间戳无法保证与实际事件顺序一致（例如由于时钟偏移）。
  
  不规范的quorum(参阅第5章“ 宽松的quorum与数据回传) 也会破坏线性化。甚至即使是严格的quorum, 正如之后即将介绍的， 也会发生违背线性化的情况。

##### 线性化与 quorum



#### 线性化的代价



##### CAP 理论









